
"""
Complete GRIT Processing Pipeline - ALL REGIONS
================================================

Process GRIT v0.6 data for ALL regions: AF, AS, EU, NA, SA, SI, SP
End-to-end: segments ‚Üí reaches ‚Üí surface areas

Usage:
    python scripts/process_grit_all_regions.py
    python scripts/process_grit_all_regions.py --regions AS EU  # Specific regions
    python scripts/process_grit_all_regions.py --skip-reaches
"""

import sys
import warnings
import time
import argparse
from pathlib import Path
from typing import Optional, Dict, List
import pandas as pd
import geopandas as gpd
import numpy as np

warnings.filterwarnings('ignore')

# ==============================================================================
# CONFIGURATION
# ==============================================================================
BASE_DIR = Path(__file__).resolve().parent.parent
GRIT_DIR = BASE_DIR / 'data' / 'raw' / 'GRIT-Michel_2025'
PROCESSED_DIR = BASE_DIR / 'data' / 'processed'
OUTPUT_DIR = BASE_DIR / 'data' / 'processed'
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# All GRIT regions
GRIT_REGIONS = {
    'AF': 'Africa',
    'AS': 'Asia (excl. Siberia)',
    'EU': 'Europe',
    'NA': 'North America',
    'SA': 'South America',
    'SI': 'Siberia',
    'SP': 'South Pacific/Australia'
}

# OSM Water (global file)
OSM_WATER_RAW = BASE_DIR / 'data' / 'raw' / 'OSM-Water-Layer-Yamazaki_2021' / 'OSM_WaterLayer_POLYGONS.gpkg'

def get_grit_files(region_code: str) -> Dict[str, Path]:
    """Get GRIT file paths for a region"""
    return {
        'segments': GRIT_DIR / f'GRITv06_segments_{region_code}_EPSG4326.gpkg',
        'reaches': GRIT_DIR / f'GRITv06_reaches_{region_code}_EPSG4326.gpkg',
        'catchments': GRIT_DIR / f'GRITv06_component_catchments_{region_code}_EPSG4326.gpkg'
    }

def print_section(title: str):
    """Print section header"""
    print(f"\n{'='*80}")
    print(f"{title}")
    print(f"{'='*80}")

# ==============================================================================
# PROCESSING FUNCTIONS
# ==============================================================================

def process_segments(region_code: str, grit_files: Dict[str, Path]) -> Optional[gpd.GeoDataFrame]:
    """
    Classify GRIT segments using SALINITY DATA (scientifically correct method)
    
    CRITICAL: Component catchments' is_coastal=1 means "drains to ocean" (geographic),
    NOT "is estuarine" (chemical/ecological). Using spatial intersection with coastal
    catchments creates massive false positives (e.g., Paris, Cologne flagged as estuarine).
    
    Estuarine classification MUST use salinity (Venice System: 0.5-30 PSU).
    """
    print(f"\nüìä STEP 1: Processing Segments")
    print(f"-" * 80)
    
    segments_file = grit_files['segments']
    
    if not segments_file.exists():
        print(f"‚ùå Segments file not found: {segments_file.name}")
        return None
    
    # Load segments
    print(f"üìÇ Loading segments: {segments_file.name}")
    start_time = time.time()
    segments = gpd.read_file(segments_file, layer='lines', engine='pyogrio')
    elapsed = time.time() - start_time
    print(f"‚úì Loaded {len(segments):,} segments in {elapsed:.1f}s")
    
    # Check for salinity data (scientifically correct method)
    salinity_file = PROCESSED_DIR / 'globsalt_salinity_zones.gpkg'
    
    if salinity_file.exists():
        print(f"\nüíß Classifying using SALINITY DATA (Venice System)")
        print(f"   (Scientifically correct: estuarine = 0.5-30 PSU)")
        
        try:
            salinity = gpd.read_file(salinity_file)
            print(f"‚úì Loaded {len(salinity):,} salinity zones")
            
            # FIX 1: Ensure CRS compatibility (CRITICAL!)
            if segments.crs != salinity.crs:
                print(f"   ‚ö†Ô∏è  CRS mismatch! Reprojecting salinity zones...")
                print(f"      Segments CRS: {segments.crs}")
                print(f"      Salinity CRS: {salinity.crs}")
                salinity = salinity.to_crs(segments.crs)
                print(f"   ‚úì Reprojected salinity zones to {segments.crs}")
            else:
                print(f"   ‚úì CRS match: {segments.crs}")
            
            # Spatial join with salinity zones
            print(f"\n   Performing spatial join (segments √ó salinity zones)...")
            print(f"   This may take 5-15 minutes for large regions...")
            segments_with_salinity = gpd.sjoin(
                segments,
                salinity[['salinity_mean_psu', 'salinity_zone', 'geometry']],
                how='left',
                predicate='intersects'
            )
            
            # FIX 2: Handle duplicate matches from overlapping buffers (CRITICAL!)
            # Segments crossing multiple 10km zones will have multiple rows
            print(f"   Segments before deduplication: {len(segments_with_salinity):,}")
            
            # Take MAXIMUM salinity per segment (conservative approach)
            segments_with_salinity = segments_with_salinity.sort_values(
                'salinity_mean_psu', ascending=True, na_position='first'
            ).drop_duplicates(subset=['global_id'], keep='last')
            
            print(f"   ‚úì After deduplication: {len(segments_with_salinity):,} unique segments")
            
            # Verify we didn't lose segments
            if len(segments_with_salinity) != len(segments):
                print(f"   ‚ö†Ô∏è  WARNING: Segment count mismatch!")
                print(f"      Original: {len(segments):,}, After join: {len(segments_with_salinity):,}")
                # Re-merge to ensure all segments retained
                segments_with_salinity = segments.merge(
                    segments_with_salinity[['global_id', 'salinity_mean_psu', 'salinity_zone']],
                    on='global_id',
                    how='left'
                )
            
            # Classify based on Venice System (international standard)
            def classify_by_salinity(row):
                sal = row.get('salinity_mean_psu', np.nan)
                if pd.isna(sal):
                    return 'Freshwater (No Salinity Data)'
                elif sal < 0.5:
                    return 'Freshwater (Non-Tidal)'
                elif sal < 5.0:
                    return 'Oligohaline (Tidal Freshwater Zone)'
                elif sal < 18.0:
                    return 'Mesohaline (Upper Estuary)'
                elif sal < 30.0:
                    return 'Polyhaline (Lower Estuary)'
                else:
                    return 'Euhaline (Marine)'
            
            segments_with_salinity['system_type'] = segments_with_salinity.apply(
                classify_by_salinity, axis=1
            )
            
            matched = segments_with_salinity['salinity_mean_psu'].notna().sum()
            print(f"\n‚úì Segments with salinity data: {matched:,} / {len(segments):,} ({matched/len(segments)*100:.1f}%)")
            
            segments_classified = segments_with_salinity
            
        except Exception as e:
            print(f"‚ö†Ô∏è  Error loading salinity data: {e}")
            print(f"   Falling back to conservative classification")
            segments_classified = segments.copy()
            segments_classified['system_type'] = 'Freshwater (Classification Pending)'
    
    else:
        # CONSERVATIVE FALLBACK: No salinity data available
        print(f"\n‚ö†Ô∏è  WARNING: No salinity data available!")
        print(f"   Cannot scientifically classify estuarine systems without salinity")
        print(f"   Classifying ALL segments as 'Freshwater (Pending)'")
        print(f"")
        print(f"   To enable proper estuarine classification:")
        print(f"     1. Process GlobSalt data: python scripts/process_globsalt_integrated.py")
        print(f"     2. Verify: {salinity_file}")
        print(f"     3. Rerun this script")
        print(f"")
        print(f"   ‚ö†Ô∏è  SCIENTIFIC NOTE: Component catchments' is_coastal=1 means")
        print(f"      'drains to ocean' (geographic), NOT 'is estuarine' (chemical).")
        print(f"      Using is_coastal would misclassify Paris, Cologne as estuarine!")
        print(f"")
        print(f"   üí° FUTURE ENHANCEMENT: Could use GRIT nodes (coastal_outlet type)")
        print(f"      to compute network distance from coast as fallback method.")
        
        segments_classified = segments.copy()
        segments_classified['system_type'] = 'Freshwater (Classification Pending)'
    
    # Summary
    print(f"\nüìä Classification Summary:")
    for sys_type, count in segments_classified['system_type'].value_counts().items():
        print(f"    {sys_type}: {count:,} segments ({count/len(segments_classified)*100:.1f}%)")
    
    # Data coverage notice
    if 'salinity_mean_psu' in segments_classified.columns:
        matched_pct = segments_classified['salinity_mean_psu'].notna().sum() / len(segments_classified) * 100
        if matched_pct < 10:
            print(f"\n‚ö†Ô∏è  DATA COVERAGE NOTICE:")
            print(f"   GlobSalt provides direct salinity for only {matched_pct:.1f}% of segments")
            print(f"   This is expected: GlobSalt has 22,937 stations for millions of river km globally")
            print(f"   Remaining segments conservatively classified as 'No Salinity Data'")
            print(f"   This is scientifically honest - we only classify what we can validate!")
            print(f"")
            print(f"   üí° Coverage varies by region:")
            print(f"      ‚Ä¢ Europe/Asia: 5-20% (better monitoring)")
            print(f"      ‚Ä¢ Americas: 2-10%")
            print(f"      ‚Ä¢ Oceania/Africa: 0.5-5% (sparse stations)")
            print(f"")
            print(f"   For improved classification, future work could integrate:")
            print(f"      1. GRIT coastal outlet proximity (network distance)")
            print(f"      2. D√ºrr 2011 estuary database (already in data/raw/)")
            print(f"      3. Regional salinity models")
    
    # Save
    output_file = OUTPUT_DIR / f'rivers_grit_segments_classified_{region_code.lower()}.gpkg'
    print(f"\nüíæ Saving: {output_file.name}")
    segments_classified.to_file(output_file, driver='GPKG')
    print(f"‚úì Saved: {output_file}")
    
    return segments_classified


def process_reaches(region_code: str, segments: gpd.GeoDataFrame, grit_files: Dict[str, Path]) -> Optional[gpd.GeoDataFrame]:
    """Load and classify GRIT reaches"""
    print(f"\nüìä STEP 2: Processing Reaches")
    print(f"-" * 80)
    
    reaches_file = grit_files['reaches']
    
    if not reaches_file.exists():
        print(f"‚ùå Reaches file not found: {reaches_file.name}")
        return None
    
    # Load reaches
    print(f"üìÇ Loading reaches: {reaches_file.name}")
    start_time = time.time()
    reaches = gpd.read_file(reaches_file, layer='lines', engine='pyogrio')
    elapsed = time.time() - start_time
    print(f"‚úì Loaded {len(reaches):,} reaches in {elapsed:.1f}s")
    print(f"    With width data: {reaches['grwl_width_median'].notna().sum():,} reaches")
    
    # Inherit classification from segments
    print(f"\nüîó Joining with segments (segment_id)...")
    
    # Select columns that actually exist in segments
    # Note: 'is_coastal' is in catchments, not segments after our salinity-based classification
    segment_cols = ['global_id', 'system_type']
    if 'salinity_mean_psu' in segments.columns:
        segment_cols.append('salinity_mean_psu')
    
    reaches_classified = reaches.merge(
        segments[segment_cols],
        left_on='segment_id',
        right_on='global_id',
        how='left',
        suffixes=('', '_segment')
    )
    
    matched = reaches_classified['system_type'].notna().sum()
    print(f"‚úì Matched: {matched:,} / {len(reaches):,} reaches ({matched/len(reaches)*100:.1f}%)")
    
    # Summary
    print(f"\nüìä Classification Summary:")
    for sys_type, count in reaches_classified['system_type'].value_counts().items():
        print(f"    {sys_type}: {count:,} reaches ({count/len(reaches_classified)*100:.1f}%)")
    
    # Save
    output_file = OUTPUT_DIR / f'rivers_grit_reaches_classified_{region_code.lower()}.gpkg'
    print(f"\nüíæ Saving: {output_file.name}")
    reaches_classified.to_file(output_file, driver='GPKG')
    print(f"‚úì Saved: {output_file}")
    
    return reaches_classified


def calculate_statistics(region_code: str, reaches: gpd.GeoDataFrame) -> Dict:
    """Calculate regional statistics"""
    print(f"\nüìä STEP 3: Calculating Statistics")
    print(f"-" * 80)
    
    stats = {
        'region': region_code,
        'total_reaches': len(reaches),
        'total_segments': reaches['segment_id'].nunique(),
        'total_length_km': reaches['length'].sum() / 1000,
    }
    
    # By system type
    for sys_type in reaches['system_type'].unique():
        subset = reaches[reaches['system_type'] == sys_type]
        stats[f'{sys_type}_reaches'] = len(subset)
        stats[f'{sys_type}_length_km'] = subset['length'].sum() / 1000
    
    # Width statistics
    with_width = reaches[reaches['grwl_width_median'].notna()]
    if len(with_width) > 0:
        stats['reaches_with_width'] = len(with_width)
        stats['avg_width_m'] = with_width['grwl_width_median'].mean()
        stats['max_width_m'] = with_width['grwl_width_median'].max()
    
    print(f"\nüìä Regional Statistics:")
    print(f"    Total reaches: {stats['total_reaches']:,}")
    print(f"    Total length: {stats['total_length_km']:,.0f} km")
    if 'reaches_with_width' in stats:
        print(f"    With width data: {stats['reaches_with_width']:,}")
        print(f"    Average width: {stats['avg_width_m']:.1f} m")
    
    return stats


# ==============================================================================
# MAIN PIPELINE
# ==============================================================================

def process_region(region_code: str, skip_reaches: bool = False) -> Dict:
    """Process a single region"""
    region_name = GRIT_REGIONS[region_code]
    
    print_section(f"üåç PROCESSING: {region_name.upper()} ({region_code})")
    print(f"Started: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    start_time = time.time()
    grit_files = get_grit_files(region_code)
    
    # Check files exist
    print(f"\nüìÅ Checking files...")
    all_exist = True
    for name, path in grit_files.items():
        if path.exists():
            size_mb = path.stat().st_size / (1024**2)
            print(f"    ‚úì {name}: {path.name} ({size_mb:.0f} MB)")
        else:
            print(f"    ‚úó {name}: {path.name} (NOT FOUND)")
            all_exist = False
    
    if not all_exist:
        print(f"\n‚ö†Ô∏è  Skipping {region_name} - missing files")
        return {'region': region_code, 'status': 'skipped', 'reason': 'missing_files'}
    
    # Process segments
    segments = process_segments(region_code, grit_files)
    if segments is None:
        return {'region': region_code, 'status': 'failed', 'step': 'segments'}
    
    # Process reaches
    if not skip_reaches:
        reaches = process_reaches(region_code, segments, grit_files)
        if reaches is None:
            return {'region': region_code, 'status': 'failed', 'step': 'reaches'}
        
        # Calculate stats
        stats = calculate_statistics(region_code, reaches)
    else:
        print(f"\n‚è≠Ô∏è  Skipping reaches processing")
        stats = {'region': region_code, 'status': 'segments_only'}
    
    elapsed = time.time() - start_time
    print(f"\n‚úÖ {region_name} complete in {elapsed/60:.1f} minutes")
    
    return {'region': region_code, 'status': 'success', 'time_minutes': elapsed/60, **stats}


def main():
    """Main pipeline - process all regions"""
    parser = argparse.ArgumentParser(description='Process GRIT data for all regions')
    parser.add_argument('--regions', nargs='+', choices=list(GRIT_REGIONS.keys()), 
                        help='Specific regions to process (default: all)')
    parser.add_argument('--skip-reaches', action='store_true', 
                        help='Skip reaches processing (segments only)')
    args = parser.parse_args()
    
    print_section("üåç GRIT GLOBAL PROCESSING PIPELINE")
    print(f"Processing ALL {len(GRIT_REGIONS)} regions")
    print(f"Started: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Determine regions to process
    regions_to_process = args.regions if args.regions else list(GRIT_REGIONS.keys())
    
    print(f"\nüìã Regions to process: {', '.join(regions_to_process)}")
    
    # Process each region
    all_results = []
    script_start = time.time()
    
    for region_code in regions_to_process:
        result = process_region(region_code, skip_reaches=args.skip_reaches)
        all_results.append(result)
    
    # Global summary
    total_time = time.time() - script_start
    
    print_section("üéâ GLOBAL PROCESSING COMPLETE")
    print(f"Total time: {total_time/60:.1f} minutes ({total_time/3600:.2f} hours)")
    
    print(f"\nüìä Regional Summary:")
    for result in all_results:
        status_icon = "‚úÖ" if result['status'] == 'success' else "‚ö†Ô∏è" if result['status'] == 'skipped' else "‚ùå"
        region_name = GRIT_REGIONS[result['region']]
        if result['status'] == 'success':
            print(f"    {status_icon} {region_name:30s}: {result.get('total_reaches', 0):>10,} reaches")
        else:
            print(f"    {status_icon} {region_name:30s}: {result['status']}")
    
    # Save global summary
    summary_df = pd.DataFrame(all_results)
    summary_file = OUTPUT_DIR / 'grit_global_processing_summary.csv'
    summary_df.to_csv(summary_file, index=False)
    print(f"\nüíæ Summary saved: {summary_file}")
    
    print(f"\n‚úÖ All processing complete!")
    print(f"    Output files: {OUTPUT_DIR}")
    
    return 0


if __name__ == '__main__':
    sys.exit(main())
